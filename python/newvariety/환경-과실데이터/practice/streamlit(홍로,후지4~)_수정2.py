# -*- coding: utf-8 -*-
# íŒŒì¼ëª…: ê¸°ìƒìë™ë¶ˆëŸ¬ì˜¤ê¸°3.py
import streamlit as st
import pandas as pd
import numpy as np
import requests
from datetime import datetime
import io
import matplotlib.pyplot as plt
from typing import Optional
import re

import matplotlib
import matplotlib.font_manager as fm
import os

def _set_korean_font():
    bundled_candidates = [
        "fonts/NanumGothic.ttf",
        "fonts/NotoSansKR-Regular.otf",
        "NanumGothic.ttf",
        "NotoSansKR-Regular.otf",
    ]
    for p in bundled_candidates:
        if os.path.exists(p):
            try:
                fm.fontManager.addfont(p)
                family = fm.FontProperties(fname=p).get_name()
                matplotlib.rcParams["font.family"] = family
                matplotlib.rcParams["axes.unicode_minus"] = False
                return
            except Exception:
                pass
    preferred = ["Malgun Gothic", "AppleGothic", "NanumGothic",
                 "Noto Sans CJK KR", "Noto Sans KR", "NanumBarunGothic"]
    sys_fonts = set(f.name for f in fm.fontManager.ttflist)
    for name in preferred:
        if name in sys_fonts:
            matplotlib.rcParams["font.family"] = name
            matplotlib.rcParams["axes.unicode_minus"] = False
            return
    matplotlib.rcParams["axes.unicode_minus"] = False
    st.warning("í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. fonts/ í´ë”ì— ë‚˜ëˆ”ê³ ë”•(NanumGothic.ttf) ë“±ì„ ë„£ì–´ì£¼ì„¸ìš”.")

_set_korean_font()

# ---------------------------
# ê³µí†µ ìœ í‹¸: í’ˆì§ˆí‘œ ì»¬ëŸ¼ ì •ê·œí™”/íŒ¨í„´ë§¤ì¹­
# ---------------------------
def normalize_quality_columns(df: pd.DataFrame) -> pd.DataFrame:
    """NBSP/ì¤„ë°”ê¿ˆ/ê´„í˜¸ ê³µë°±/ë©€í‹°í—¤ë” ë“±ì„ í†µì¼"""
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = [" ".join([str(x) for x in tup if str(x) != "nan"]).strip()
                      for tup in df.columns.values]
    def _clean(s: str) -> str:
        s = str(s)
        s = s.replace("\xa0", " ")              # NBSP -> space
        s = s.replace("\n", " ").replace("\r", " ")
        s = re.sub(r"\s+", " ", s).strip()
        s = re.sub(r"\s*\(\s*", " (", s)        # "( " -> " ("
        s = re.sub(r"\s*\)\s*", ")", s)         # " )" -> ")"
        return s
    out = df.copy()
    out.columns = [_clean(c) for c in out.columns]
    # ìì£¼ ë³´ì´ëŠ” ë³€í˜• ë³„ì¹­ë„ í•œ ë²ˆ ë” ì •ëˆ(ì„ íƒ)
    alias = {
        "ê²½ë„ í‰ê· (N/Ã¸11mm)": "ê²½ë„í‰ê· (N/Ã¸11mm)",
        "ê²½ë„ í‰ê·  (N/Ã¸11mm)": "ê²½ë„í‰ê· (N/Ã¸11mm)",
        "ì°©ìƒ‰ (Hunter L)": "ì°©ìƒ‰(Hunter L)",
        "ì°©ìƒ‰ (Hunter a)": "ì°©ìƒ‰(Hunter a)",
        "ì°©ìƒ‰ (Hunter b)": "ì°©ìƒ‰(Hunter b)",
    }
    out = out.rename(columns={k: v for k, v in alias.items() if k in out.columns})
    return out

def get_first_col_by_pattern(df: pd.DataFrame, pattern: str) -> Optional[str]:
    """ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ ì»¬ëŸ¼ëª… 1ê°œ ì°¾ê¸°(ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)"""
    pat = re.compile(pattern, flags=re.IGNORECASE)
    for c in df.columns:
        if pat.search(str(c)):
            return c
    return None

# -------------------------------------------------
# ê¸°ë³¸ UI
# -------------------------------------------------
st.set_page_config(page_title="ğŸ ì‚¬ê³¼ ê¸°ìƒ í†µê³„ + íšŒê·€ì‹ ì˜ˆì¸¡", layout="wide")
st.title("ğŸ ì‚¬ê³¼ ê¸°ìƒ í†µê³„ ìˆ˜ì§‘ + íšŒê·€ì‹ ì˜ˆì¸¡ê¸°")

# -------------------------------------------------
# ì§€ì—­ ì½”ë“œ (ì‚¬ì´íŠ¸ì˜ select valueì™€ ë™ì¼)
# -------------------------------------------------
AREA_CODE = {
    "ê²½ê¸°í™”ì„±": "324",
    "ê²½ë¶ì˜ì£¼": "331",
    "ê²½ë¶ì²­ì†¡": "332",
    "ëŒ€êµ¬êµ°ìœ„": "333",
    "ê²½ë‚¨ê±°ì°½": "334",
    "ì „ë¶ì¥ìˆ˜": "335",
    "ê²½ê¸°í¬ì²œ": "337",
    "ì¶©ë¶ì¶©ì£¼": "338",
}

# ê³¼ì‹¤í’ˆì§ˆ í˜ì´ì§€ì˜ ì§€ì—­ í‘œê¸°ì™€ ë§¤í•‘
REGION_NAME_MAP = {
    "ê²½ê¸°í™”ì„±": "í™”ì„±",
    "ê²½ë¶ì˜ì£¼": "ì˜ì£¼",
    "ê²½ë¶ì²­ì†¡": "ì²­ì†¡",
    "ëŒ€êµ¬êµ°ìœ„": "êµ°ìœ„",
    "ê²½ë‚¨ê±°ì°½": "ê±°ì°½",
    "ì „ë¶ì¥ìˆ˜": "ì¥ìˆ˜",
    "ê²½ê¸°í¬ì²œ": "í¬ì²œ",
    "ì¶©ë¶ì¶©ì£¼": "ì¶©ì£¼",
}

# -------------------------------------------------
# íšŒê·€ì‹ í•˜ë“œì½”ë”©: í’ˆì¢…ë³„
#   ì¤‘ê°„ì  'Â·'ì€ ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ '*'ë¡œ ì¹˜í™˜
# -------------------------------------------------
EQUATIONS_BY_CULTIVAR = {
    "í™ë¡œ": {
        "ê³¼ì¤‘": "ê³¼ì¤‘ = 667.243 -0.597465Â·ì¼ì‚¬ëŸ‰_sum_m06 -0.376337Â·ì¼ì‚¬ëŸ‰_sum_m04 +0.0756442Â·ì¼ì‚¬ëŸ‰_sum_m08 +24.5342Â·í‰ê· í’ì†_mean_m06 +5.05212Â·ìµœì €ê¸°ì˜¨_mean_m06",
        "ì¢…ê²½": "ì¢…ê²½ = 124.046 -0.0439843Â·ì¼ì‚¬ëŸ‰_sum_m04 -0.0673823Â·ì¼ì‚¬ëŸ‰_sum_m06 +0.397906Â·ìµœì €ê¸°ì˜¨_mean_m07 +0.559339Â·í‰ê· í’ì†_mean_m06 +7.43416Â·í‰ê· í’ì†_mean_m05 +0.00854761Â·ì¼ì‚¬ëŸ‰_sum_m08 -0.15116Â·ìµœì €ê¸°ì˜¨_mean_m06 +1.05474Â·ìµœì €ê¸°ì˜¨_mean_m05 -2.574Â·ìµœëŒ€í’ì†_mean_m04",
        "íš¡ê²½": "íš¡ê²½ = 141.358 -0.0531011Â·ì¼ì‚¬ëŸ‰_sum_m06 -0.0459844Â·ì¼ì‚¬ëŸ‰_sum_m04 -0.00297393Â·ì¼ì‚¬ëŸ‰_sum_m08",
        "L":   "L = -68.3528 +2.1686e-05Â·ê°•ìš°ëŸ‰_sum_m07 +0.653086Â·ìŠµë„_mean_m07 +2.72693Â·ìµœì €ê¸°ì˜¨_mean_m06 +1.14857Â·í‰ê· ê¸°ì˜¨_mean_m08 +0.00918464Â·ê°•ìš°ëŸ‰_sum_m06 -1.52125Â·ìµœì €ê¸°ì˜¨_mean_m05 -6.00147Â·í‰ê· í’ì†_mean_m06 +3.14509Â·ìµœëŒ€í’ì†_mean_m06 +4.16545Â·í‰ê· í’ì†_mean_m07",
        "a":   "a = 226.087 -1.71711Â·ìŠµë„_mean_m07 +0.00830904Â·ê°•ìš°ëŸ‰_sum_m07 -4.51272Â·ìµœì €ê¸°ì˜¨_mean_m06 +1.51756Â·ìµœì €ê¸°ì˜¨_mean_m05 -1.94971Â·í‰ê· ê¸°ì˜¨_mean_m08 -0.040713Â·ì¼ì‚¬ëŸ‰_sum_m07 -0.0185248Â·ê°•ìš°ëŸ‰_sum_m06 +0.00975096Â·ê°•ìš°ëŸ‰_sum_m08 +1.93026Â·ìµœê³ ê¸°ì˜¨_mean_m07 -0.192988Â·í‰ê· í’ì†_mean_m06",
        "b":   "b = -23.7933 +0.381237Â·ìŠµë„_mean_m07 +0.0052134Â·ê°•ìš°ëŸ‰_sum_m06 +0.0606139Â·ìŠµë„_mean_m05 +1.14817Â·ìµœì €ê¸°ì˜¨_mean_m06 +0.682908Â·í‰ê· í’ì†_mean_m07 -0.523353Â·ìµœì €ê¸°ì˜¨_mean_m05 -0.350293Â·ìµœê³ ê¸°ì˜¨_mean_m05 -0.00264168Â·ê°•ìš°ëŸ‰_sum_m08 -1.23413Â·í‰ê· í’ì†_mean_m08 +0.652516Â·ìµœëŒ€í’ì†_mean_m06",
        "ê²½ë„": "ê²½ë„ = 54.6658 +0.2039Â·ìŠµë„_mean_m04 +0.0144323Â·ì¼ì‚¬ëŸ‰_sum_m08 -0.194462Â·ìŠµë„_mean_m08 -0.0140798Â·ì¼ì‚¬ëŸ‰_sum_m04 +0.00218425Â·ê²°ë¡œì‹œê°„_mean_m04 +0.364872Â·í‰ê· ê¸°ì˜¨_mean_m08",
        "ë‹¹ë„": "ë‹¹ë„ = 1.14467 -0.425354Â·ìµœëŒ€í’ì†_mean_m06 -1.03279Â·í‰ê· í’ì†_mean_m07 -0.0754722Â·í‰ê· í’ì†_mean_m08 +0.781233Â·í‰ê· í’ì†_mean_m04 -0.0277847Â·ìµœê³ ê¸°ì˜¨_mean_m08 -0.0127413Â·ìŠµë„_mean_m05 +0.0022906Â·ê²°ë¡œì‹œê°„_mean_m05 +0.259103Â·ìµœê³ ê¸°ì˜¨_mean_m06 +0.0923847Â·ìŠµë„_mean_m04 +0.410232Â·ìµœëŒ€í’ì†_mean_m04 -0.00215038Â·ê°•ìš°ëŸ‰_sum_m06",
        "ì‚°ë„": "ì‚°ë„ = 0.262689 +0.0555189Â·ìµœëŒ€í’ì†_mean_m06 +0.0451885Â·í‰ê· í’ì†_mean_m08 -0.0549304Â·í‰ê· í’ì†_mean_m04 -0.00534754Â·ìµœê³ ê¸°ì˜¨_mean_m06 -0.0236952Â·í‰ê· í’ì†_mean_m07 -0.00264247Â·ìŠµë„_mean_m04 +0.00413186Â·ìŠµë„_mean_m08 -0.00334177Â·í‰ê· ê¸°ì˜¨_mean_m07 +0.000124634Â·ê°•ìš°ëŸ‰_sum_m06 -0.001393Â·ìŠµë„_mean_m05",
    },
    "í›„ì§€": {
        "ê³¼ì¤‘": "ê³¼ì¤‘ = 399.484 -0.229845Â·ì¼ì‚¬ëŸ‰_sum_m04 +6.76485Â·ìµœì €ê¸°ì˜¨_mean_m05 -17.8404Â·ìµœëŒ€í’ì†_mean_m07",
        "ì¢…ê²½": "ì¢…ê²½ = 183.553 -0.0439139Â·ì¼ì‚¬ëŸ‰_sum_m04 -0.626045Â·ìµœì €ê¸°ì˜¨_mean_m04 -0.0145561Â·ì¼ì‚¬ëŸ‰_sum_m10 +0.955631Â·ìµœì €ê¸°ì˜¨_mean_m05 -0.0373121Â·í‰ê· ê¸°ì˜¨_mean_m10 -0.656449Â·ìŠµë„_mean_m08 -0.0131851Â·ìŠµë„_mean_m06 +1.16239Â·í‰ê· ê¸°ì˜¨_mean_m07 -1.16892Â·ìµœì €ê¸°ì˜¨_mean_m09 -0.420997Â·ìŠµë„_mean_m09",
        "íš¡ê²½": "íš¡ê²½ = 79.6808 -1.82161Â·ìµœëŒ€í’ì†_mean_m07 +0.796471Â·í‰ê· ê¸°ì˜¨_mean_m05",
        "L":   "L = 75.7441 -0.134414Â·ìŠµë„_mean_m08 -0.400198Â·í‰ê· ê¸°ì˜¨_mean_m10 +0.0159958Â·ê°•ìš°ëŸ‰_sum_m10 +0.0240315Â·ì¼ì‚¬ëŸ‰_sum_m08 +0.812706Â·ìµœì €ê¸°ì˜¨_mean_m04 -0.0812023Â·ìŠµë„_mean_m04 -0.206954Â·ìŠµë„_mean_m06 +0.116267Â·ìŠµë„_mean_m10 -0.0069829Â·ì¼ì‚¬ëŸ‰_sum_m04 -2.452Â·í‰ê· í’ì†_mean_m04 -0.0989298Â·í‰ê· ê¸°ì˜¨_mean_m08 -0.384125Â·í‰ê· ê¸°ì˜¨_mean_m07",
        "a":   "a = 4.0922 -0.0203282Â·ê°•ìš°ëŸ‰_sum_m10 -2.85393Â·ìµœëŒ€í’ì†_mean_m05 -0.00910066Â·ì¼ì‚¬ëŸ‰_sum_m08 +0.0392451Â·í‰ê· ê¸°ì˜¨_mean_m06 +0.307386Â·ìŠµë„_mean_m08 +0.000860505Â·ê°•ìš°ëŸ‰_sum_m08 -0.688696Â·ìµœëŒ€í’ì†_mean_m08 -0.00144964Â·ì¼ì‚¬ëŸ‰_sum_m05 +0.758309Â·ìµœëŒ€í’ì†_mean_m07",
        "b":   "b = 12.841 -0.0308293Â·ìŠµë„_mean_m08 +0.0145751Â·ì¼ì‚¬ëŸ‰_sum_m08 -0.00322966Â·ê°•ìš°ëŸ‰_sum_m08 +0.035395Â·í‰ê· ê¸°ì˜¨_mean_m10 +0.0796701Â·ìŠµë„_mean_m10 -0.000543608Â·ì¼ì‚¬ëŸ‰_sum_m06 +0.00337387Â·ê°•ìš°ëŸ‰_sum_m10 -0.00372859Â·ì¼ì‚¬ëŸ‰_sum_m04 -0.14243Â·ìŠµë„_mean_m06 +0.0641568Â·í‰ê· ê¸°ì˜¨_mean_m08 +0.14721Â·ìµœì €ê¸°ì˜¨_mean_m07 +0.53868Â·í‰ê· í’ì†_mean_m04",
        "ê²½ë„": "ê²½ë„ = 8.39888 +0.0529999Â·ì¼ì‚¬ëŸ‰_sum_m09 +6.94666Â·í‰ê· í’ì†_mean_m07 +3.98929Â·ìµœëŒ€í’ì†_mean_m08 -0.0451264Â·ì¼ì‚¬ëŸ‰_sum_m08 +0.406065Â·ìŠµë„_mean_m04 -3.47701Â·í‰ê· í’ì†_mean_m06 -0.00806023Â·ê²°ë¡œì‹œê°„_mean_m10 +0.0392251Â·ì¼ì‚¬ëŸ‰_sum_m06 +0.00773583Â·ê²°ë¡œì‹œê°„_mean_m09 -2.0759Â·ìµœëŒ€í’ì†_mean_m10 +0.000289527Â·ê²°ë¡œì‹œê°„_mean_m06 -2.8229Â·ìµœëŒ€í’ì†_mean_m05 +0.000106158Â·ê²°ë¡œì‹œê°„_mean_m05 +0.270037Â·ìµœê³ ê¸°ì˜¨_mean_m09",
        "ë‹¹ë„": "ë‹¹ë„ = 10.492 +0.00486017Â·ì¼ì‚¬ëŸ‰_sum_m06 +0.00146432Â·ê²°ë¡œì‹œê°„_mean_m10 +0.00262004Â·ê²°ë¡œì‹œê°„_mean_m07 -0.00156465Â·ê²°ë¡œì‹œê°„_mean_m09 -1.20735Â·í‰ê· í’ì†_mean_m10 -0.000317261Â·ê²°ë¡œì‹œê°„_mean_m05",
        "ì‚°ë„": "ì‚°ë„ = 0.766184 -0.0175941Â·í‰ê· ê¸°ì˜¨_mean_m10 -0.00379855Â·ìŠµë„_mean_m04 -0.00807644Â·ìµœê³ ê¸°ì˜¨_mean_m04 -4.6679e-05Â·ê°•ìš°ëŸ‰_sum_m08 +0.00318949Â·ìµœê³ ê¸°ì˜¨_mean_m08 -8.77968e-05Â·ê°•ìš°ëŸ‰_sum_m10 -0.00456198Â·í‰ê· í’ì†_mean_m04 +0.00411344Â·ìµœê³ ê¸°ì˜¨_mean_m07",
    },
}

# -------------------------------------------------
# ìœ í‹¸
# -------------------------------------------------
def _ensure_numeric(df: pd.DataFrame, cols):
    for c in cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    return df

def cultivar_window(cultivar: str):
    if cultivar == "í™ë¡œ":
        return 4, 8
    return 4, 10  # í›„ì§€ ê¸°ë³¸

def get_today_ym():
    now = datetime.now()
    return now.year, now.month

# -------------------------------------------------
# ì‚¬ì´íŠ¸ AJAX â†’ JSON (AWS í†µê³„)
# -------------------------------------------------
@st.cache_data(show_spinner=False, ttl=300)
def fetch_aws_stat(region_name: str, stat_gb_code: str, s_ym: str, e_ym: str) -> dict:
    session = requests.Session()
    session.get("https://fruit.nihhs.go.kr/apple/aws/awsStat.do", timeout=20)

    form = {
        "pageIndex": "1",
        "searchNum": "0",
        "areaName": region_name,
        "areaCode": AREA_CODE.get(region_name, region_name),
        "statmethod": stat_gb_code,
    }
    if stat_gb_code == "A":
        form["wetherDtBgn"] = f"{s_ym}-01"
        form["wetherDtEnd"] = f"{e_ym}-30"
    elif stat_gb_code in ("B", "C"):
        form["wetherDtM"] = s_ym
    elif stat_gb_code == "D":
        form["wetherDtBgn2"] = s_ym
        form["wetherDtEnd2"] = e_ym
        form["wetherDtBgn"] = s_ym
        form["wetherDtEnd"] = e_ym

    resp = session.post(
        "https://fruit.nihhs.go.kr/apple/aws/awsStatList.do",
        data=form, timeout=30,
        headers={"Referer": "https://fruit.nihhs.go.kr/apple/aws/awsStat.do"}
    )
    resp.raise_for_status()
    try:
        return resp.json()
    except Exception:
        return {"error": "JSON íŒŒì‹± ì‹¤íŒ¨", "raw": resp.text[:4000]}

# -------------------------------------------------
# JSON â†’ í‘œ
# -------------------------------------------------
def json_to_dataframe(payload: dict) -> pd.DataFrame:
    if not payload or "result" not in payload:
        return pd.DataFrame()
    res = payload.get("result", [])
    if len(res) == 0:
        return pd.DataFrame()

    method = payload.get("mainAwsVO", {}).get("statmethod")
    raw = pd.DataFrame(res)

    with st.expander("ğŸ”§ API ì›ì‹œ ì»¬ëŸ¼ ë³´ê¸°", expanded=False):
        st.write(sorted(list(raw.columns)))

    rename_map = {
        "statsDt": "ì¼ì", "wetherDt": "ì¼ì",
        "dalyWetherAvrgTp": "í‰ê· ê¸°ì˜¨", "wetherAvgTp": "í‰ê· ê¸°ì˜¨",
        "dalyWetherMxmmTp": "ìµœê³ ê¸°ì˜¨", "wetherMaxTp": "ìµœê³ ê¸°ì˜¨",
        "dalyWetherMummTp": "ìµœì €ê¸°ì˜¨", "wetherMinTp": "ìµœì €ê¸°ì˜¨",
        "dalyWetherAvrgHd": "ìŠµë„",     "WetherAvgHd": "ìŠµë„",
        "dalyWetherTtalRainqy": "ê°•ìš°ëŸ‰", "wetherMaxRainqy": "ê°•ìš°ëŸ‰",
        "dalyWetherMxmmSolradqy": "ì¼ì‚¬ëŸ‰",
        "wetherMaxSolradqy": "ì¼ì‚¬ëŸ‰",
        "wetherSumSolradqy": "ì¼ì‚¬ëŸ‰",
        "dalyWetherMxmmCondenstime": "ê²°ë¡œì‹œê°„",
        "wetherMaxCondenstime": "ê²°ë¡œì‹œê°„",
        "wetherSumCondenstime": "ê²°ë¡œì‹œê°„",
        "dalyWetherAvrgWs": "í‰ê· í’ì†", "wetherAvgWs": "í‰ê· í’ì†",
        "dalyWetherMxmmWs": "ìµœëŒ€í’ì†", "wetherMaxWs": "ìµœëŒ€í’ì†",
        "wetherDtMonth": "ì›”", "wetherDt": "ì›”",
    }
    raw = raw.rename(columns=rename_map)

    if method == "A":
        want = ["ì¼ì","í‰ê· ê¸°ì˜¨","ìµœê³ ê¸°ì˜¨","ìµœì €ê¸°ì˜¨","ìŠµë„","ê°•ìš°ëŸ‰","ì¼ì‚¬ëŸ‰","ê²°ë¡œì‹œê°„","í‰ê· í’ì†","ìµœëŒ€í’ì†"]
    elif method == "B":
        want = ["ì¼ì","í‰ê· ê¸°ì˜¨","ìµœê³ ê¸°ì˜¨","ìµœì €ê¸°ì˜¨","ìŠµë„","ê°•ìš°ëŸ‰","ì¼ì‚¬ëŸ‰","í‰ê· í’ì†","ìµœëŒ€í’ì†"]
    elif method == "C":
        want = ["ìˆœ","í‰ê· ê¸°ì˜¨","ìµœê³ ê¸°ì˜¨","ìµœì €ê¸°ì˜¨","ìŠµë„","ê°•ìš°ëŸ‰","ì¼ì‚¬ëŸ‰","í‰ê· í’ì†","ìµœëŒ€í’ì†"] if "ìˆœ" in raw.columns \
               else ["ì¼ì","í‰ê· ê¸°ì˜¨","ìµœê³ ê¸°ì˜¨","ìµœì €ê¸°ì˜¨","ìŠµë„","ê°•ìš°ëŸ‰","ì¼ì‚¬ëŸ‰","í‰ê· í’ì†","ìµœëŒ€í’ì†"]
    else:  # "D"
        want = ["ì›”","í‰ê· ê¸°ì˜¨","ìµœê³ ê¸°ì˜¨","ìµœì €ê¸°ì˜¨","ìŠµë„","ê°•ìš°ëŸ‰","ì¼ì‚¬ëŸ‰","ê²°ë¡œì‹œê°„","í‰ê· í’ì†","ìµœëŒ€í’ì†"] \
               if "ì›”" in raw.columns else \
               ["ì¼ì","í‰ê· ê¸°ì˜¨","ìµœê³ ê¸°ì˜¨","ìµœì €ê¸°ì˜¨","ìŠµë„","ê°•ìš°ëŸ‰","ì¼ì‚¬ëŸ‰","ê²°ë¡œì‹œê°„","í‰ê· í’ì†","ìµœëŒ€í’ì†"]

    use_cols = [c for c in want if c in raw.columns]
    if not use_cols:
        return pd.DataFrame()
    df = raw[use_cols].copy()

    numcands = ["í‰ê· ê¸°ì˜¨","ìµœê³ ê¸°ì˜¨","ìµœì €ê¸°ì˜¨","ìŠµë„","ê°•ìš°ëŸ‰","ì¼ì‚¬ëŸ‰","ê²°ë¡œì‹œê°„","í‰ê· í’ì†","ìµœëŒ€í’ì†"]
    df = _ensure_numeric(df, [c for c in numcands if c in df.columns])

    if "ì¼ì" in df.columns and df["ì¼ì"].dtype != "int64":
        df["ì¼ì"] = pd.to_datetime(df["ì¼ì"], errors="coerce")
        if "ì—°ë„" not in df.columns:
            df["ì—°ë„"] = df["ì¼ì"].dt.year
        if "ì›”" not in df.columns:
            df["ì›”"] = df["ì¼ì"].dt.month
    elif "ì›”" in df.columns:
        df["ì—°ë„"] = pd.to_numeric(df["ì›”"].astype(str).str[:4], errors="coerce")
        df["ì›”"] = pd.to_numeric(df["ì›”"].astype(str).str[-2:], errors="coerce")

    if {"ì—°ë„","ì›”"}.issubset(df.columns):
        df = df.sort_values(["ì—°ë„","ì›”"]).reset_index(drop=True)
    return df

# -------------------------------------------------
# ì¼/ìˆœ â†’ ì›”ë³„ ì§‘ê³„
# -------------------------------------------------
def agg_to_monthly(df: pd.DataFrame) -> pd.DataFrame:
    if not {"ì—°ë„","ì›”"}.issubset(df.columns):
        return df.copy()
    agg_map = {
        "í‰ê· ê¸°ì˜¨":"mean","ìµœê³ ê¸°ì˜¨":"mean","ìµœì €ê¸°ì˜¨":"mean","ìŠµë„":"mean",
        "ê°•ìš°ëŸ‰":"sum","ì¼ì‚¬ëŸ‰":"sum","ê²°ë¡œì‹œê°„":"sum",
        "í‰ê· í’ì†":"mean","ìµœëŒ€í’ì†":"mean"
    }
    use_cols = {k:v for k,v in agg_map.items() if k in df.columns}
    out = df.groupby(["ì—°ë„","ì›”"], as_index=False).agg(use_cols)
    return out.sort_values(["ì—°ë„","ì›”"]).reset_index(drop=True)

# -------------------------------------------------
# ì›”ë³„ ê°€ë¡œ í™•ì¥ í”¼ì²˜ (_mean_mMM / _sum_mMM)
# -------------------------------------------------
def build_wide_month_feats(env_m: pd.DataFrame) -> pd.DataFrame:
    if not {"ì—°ë„","ì›”"}.issubset(env_m.columns):
        raise ValueError("env_mì— 'ì—°ë„','ì›”' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    num_cols = [c for c in env_m.columns if c not in ("ì—°ë„","ì›”") and pd.api.types.is_numeric_dtype(env_m[c])]
    mean_agg = env_m.groupby(["ì—°ë„","ì›”"], as_index=False)[num_cols].mean()
    sum_agg  = env_m.groupby(["ì—°ë„","ì›”"], as_index=False)[num_cols].sum()
    wide_mean = None
    for m in range(1, 13):
        sub = mean_agg[mean_agg["ì›”"] == m].drop(columns=["ì›”"]).copy()
        sub = sub.rename(columns={c: f"{c}_mean_m{m:02d}" for c in num_cols})
        wide_mean = sub if wide_mean is None else pd.merge(wide_mean, sub, on="ì—°ë„", how="outer")
    wide_sum = None
    for m in range(1, 13):
        sub = sum_agg[sum_agg["ì›”"] == m].drop(columns=["ì›”"]).copy()
        sub = sub.rename(columns={c: f"{c}_sum_m{m:02d}" for c in num_cols})
        wide_sum = sub if wide_sum is None else pd.merge(wide_sum, sub, on="ì—°ë„", how="outer")
    wide = pd.merge(wide_mean, wide_sum, on="ì—°ë„", how="outer").fillna(0)
    return wide

# -------------------------------------------------
# íšŒê·€ì‹ ì ìš© (í•œ ì—°ë„ rowì— ëŒ€í•´)
# -------------------------------------------------
def apply_equation_row(row: pd.Series, eq_str: str) -> float:
    rhs = eq_str.split("=", 1)[1].strip().replace("Â·", "*")
    cols = sorted(row.index.tolist(), key=len, reverse=True)
    expr = rhs
    for c in cols:
        expr = expr.replace(c, f"row[{repr(c)}]")
    return float(eval(expr, {"__builtins__": {}}, {"row": row, "np": np}))

# -------------------------------------------------
# ë¯¸í™•ë³´ ì›” ì±„ìš°ê¸°
# -------------------------------------------------
def fill_missing_or_future_with_climatology(env_m: pd.DataFrame, target_year: int, cultivar: str, mode: str = "last3") -> pd.DataFrame:
    need_cols = {"ì—°ë„","ì›”"}
    if not need_cols.issubset(env_m.columns):
        raise ValueError("env_mì—ëŠ” 'ì—°ë„','ì›”' ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")
    s_mon, e_mon = cultivar_window(cultivar)
    cur_year, cur_mon = get_today_ym()

    cur = env_m[env_m["ì—°ë„"] == target_year].copy()
    hist = env_m[env_m["ì—°ë„"] < target_year].copy()
    if hist.empty:
        return cur
    if mode == "last3":
        last_years = sorted(hist["ì—°ë„"].unique())[-3:]
        hist = hist[hist["ì—°ë„"].isin(last_years)]

    num_cols = [c for c in env_m.columns if c not in ("ì—°ë„","ì›”") and pd.api.types.is_numeric_dtype(env_m[c])]
    climo = hist.groupby("ì›”", as_index=False)[num_cols].mean()

    months_window = list(range(s_mon, e_mon+1))
    have = set(cur["ì›”"].tolist())
    future_cut = cur_mon if target_year == cur_year else 12
    future_months = [m for m in months_window if (target_year == cur_year and m > future_cut) or (target_year > cur_year)]
    missing_months = [m for m in months_window if m not in have]

    to_fill = sorted(set(future_months) | set(missing_months))
    if to_fill:
        fill_rows = climo[climo["ì›”"].isin(to_fill)].copy()
        if fill_rows.empty:
            climo_all = env_m[env_m["ì—°ë„"] < target_year].groupby("ì›”", as_index=False)[num_cols].mean()
            fill_rows = climo_all[climo_all["ì›”"].isin(to_fill)].copy()
        fill_rows.insert(0, "ì—°ë„", target_year)
        cur = pd.concat([cur, fill_rows], ignore_index=True, axis=0)

    cur = cur[(cur["ì›”"] >= s_mon) & (cur["ì›”"] <= e_mon)]
    cur = cur.sort_values(["ì—°ë„","ì›”"]).reset_index(drop=True)
    for c in num_cols:
        cur[c] = pd.to_numeric(cur[c], errors="coerce")
    return cur

# -------------------------------------------------
# ì „ë…„ë„ ê³¼ì‹¤í’ˆì§ˆ í…Œì´ë¸”
# -------------------------------------------------
@st.cache_data(show_spinner=False, ttl=3600)
def fetch_quality_tables(selyear: int, lastyear: int, cultivar: str) -> dict:
    code = "apple01" if cultivar == "í›„ì§€" else "apple02"
    url = "https://fruit.nihhs.go.kr/apple/qlityInfo_frutQlity.do"
    params = {
        "frtgrdCode": "apple",
        "selyear": str(selyear),
        "lastYear": str(lastyear),
        "searchGubun": code,
        "pageIndex": "1",
    }
    r = requests.get(url, params=params, timeout=30)
    r.raise_for_status()
    try:
        import io as _io
        tables = pd.read_html(_io.StringIO(r.text))
    except Exception as e:
        st.warning(f"ì „ë…„ë„ ê³¼ì‹¤í’ˆì§ˆ í‘œ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}\n"
                   "lxml/html5lib ì„¤ì¹˜ ì—¬ë¶€ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”. (pip install lxml html5lib)")
        return {}

    cleaned = []
    for t in tables:
        t2 = normalize_quality_columns(t)
        if set(["ì§€ì—­","ìˆ˜í™•ì¼ì"]).issubset(set(t2.columns)):
            cleaned.append(t2)

    result = {}
    if len(cleaned) >= 1:
        result["this"] = cleaned[0]
    if len(cleaned) >= 2:
        result["last"] = cleaned[1]
    return result

def pick_region_row(qdf: pd.DataFrame, region_disp_name: str) -> Optional[pd.Series]:
    if qdf is None or qdf.empty:
        return None
    sub = qdf[qdf["ì§€ì—­"].astype(str).str.strip() == region_disp_name]
    if sub.empty:
        return None
    sub = sub.copy()
    sub["ìˆ˜í™•ì¼ì"] = pd.to_datetime(sub["ìˆ˜í™•ì¼ì"], errors="coerce")
    sub = sub.sort_values("ìˆ˜í™•ì¼ì", ascending=False)
    return sub.iloc[0]  # âœ… ì—¬ê¸°ì„œ ë. (ì£½ì€ ì½”ë“œ ì œê±°)

# -------------------------------------------------
# ì‚¬ì´ë“œë°”
# -------------------------------------------------
with st.sidebar:
    st.header("ì¡°íšŒ ì¡°ê±´")
    region = st.selectbox("ì§€ì—­", list(AREA_CODE.keys()), index=1)
    cultivar = st.selectbox("í’ˆì¢…", ["í™ë¡œ", "í›„ì§€"], index=0)
    fill_strategy = st.selectbox("ì˜ˆìƒ ë‚ ì”¨ ë°©ë²•", ["ìµœê·¼ 3ë…„ ì›”í‰ê· ", "ì „ì²´ ê³¼ê±° ì›”í‰ê· "], index=0)
    run = st.button("ğŸ” ìë™ì¡°íšŒ & ì˜ˆì¸¡")

# -------------------------------------------------
# ì‹¤í–‰
# -------------------------------------------------
if run:
    try:
        cur_year, cur_month = get_today_ym()
        s_mon, e_mon = cultivar_window(cultivar)
        s_ym = f"{cur_year:04d}-{s_mon:02d}"
        e_ym_real = f"{cur_year:04d}-{min(e_mon, cur_month):02d}"

        with st.spinner("ê¸°ìƒ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ìˆìŠµë‹ˆë‹¤..."):
            payload = fetch_aws_stat(region, "D", s_ym, e_ym_real)

        if "error" in payload:
            st.error("ì‘ë‹µì´ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
            st.code(payload.get("raw","")[:1000])
            st.stop()

        df = json_to_dataframe(payload)
        st.subheader("{region_disp} ê¸°ìƒ ë°ì´í„°")
        if df.empty:
            st.warning("ì˜¬í•´ ê¸°ê°„ ë‚´ ì‹¤ì¸¡ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê³¼ê±° ê¸°í›„í‰ë…„ë§Œìœ¼ë¡œ ì±„ì›Œ ì˜ˆì¸¡í•©ë‹ˆë‹¤.")
        else:
            st.dataframe(df, use_container_width=True)
    except Exception as e:
        st.error("ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì•„ë˜ ì˜ˆì™¸ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        st.exception(e)

    env_m = df.copy()
    if not env_m.empty and "ì—°ë„" not in env_m.columns and "ì›”" in env_m.columns:
        env_m["ì—°ë„"] = env_m["ì›”"].astype(str).str[:4].astype(int)
        env_m["ì›”"] = env_m["ì›”"].astype(str).str[5:7].astype(int)

    past_payload = fetch_aws_stat(region, "D", f"{max(cur_year-15,2010):04d}-01", f"{cur_year-1:04d}-12")
    past_df = json_to_dataframe(past_payload)
    env_all = pd.concat([env_m, past_df], ignore_index=True) if not past_df.empty else env_m

    mode = "last3" if "ìµœê·¼ 3ë…„" in fill_strategy else "all"
    filled_this_year = fill_missing_or_future_with_climatology(env_all, cur_year, cultivar, mode=mode)

    st.subheader("{region_disp} ì˜ˆìƒ ê¸°ìƒë°ì´í„°")
    st.dataframe(filled_this_year, use_container_width=True)

    try:
        env_for_wide = pd.concat([env_all[env_all["ì—°ë„"] != cur_year], filled_this_year], ignore_index=True)
        wide = build_wide_month_feats(env_for_wide)
    except Exception as e:
        st.error(f"ì›”ë³„ í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")
        st.stop()

    if cur_year not in set(wide["ì—°ë„"].astype(int).tolist()):
        st.error("ì˜ˆì¸¡ ì—°ë„ í–‰ì„ êµ¬ì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.stop()

    row = wide[wide["ì—°ë„"] == cur_year].iloc[0]

    EQUATIONS = EQUATIONS_BY_CULTIVAR.get(cultivar, {})
    preds = {}
    for tgt, formula in EQUATIONS.items():
        try:
            preds[tgt] = apply_equation_row(row, formula)
        except Exception as e:
            preds[tgt] = f"ì—ëŸ¬: {e}"

    st.subheader(f"{cur_year} {cultivar} ê³¼ì‹¤í’ˆì§ˆì˜ˆì¸¡")
    pred_df = pd.DataFrame([preds]).T.reset_index()
    pred_df.columns = ["í•­ëª©", "ì˜ˆì¸¡ê°’(ì˜¬í•´)"]
    st.dataframe(pred_df, use_container_width=True)

    # -----------------------------
    # ì „ë…„ë„ ê³¼ì‹¤í’ˆì§ˆ í‘œ ë¶ˆëŸ¬ì™€ì„œ ì§€ì—­ í–‰ ì¶”ì¶œ
    # -----------------------------
    with st.spinner("ì „ë…„ë„ ê³¼ì‹¤í’ˆì§ˆ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
        qdict = fetch_quality_tables(cur_year, cur_year-1, cultivar)

    region_disp = REGION_NAME_MAP.get(region, region)
    last_row = None
    if qdict and "last" in qdict and qdict["last"] is not None and not qdict["last"].empty:
        q_last = normalize_quality_columns(qdict["last"])
        with st.expander("ì „ë…„ë„ í…Œì´ë¸” ì‹¤ì œ ì»¬ëŸ¼ëª…(ì •ê·œí™” í›„)"):
            st.write(list(q_last.columns))
        last_row = pick_region_row(q_last, region_disp)

    if last_row is not None:
        # íŒ¨í„´ ê¸°ë°˜ ë§¤ì¹­ìœ¼ë¡œ ì»¬ëŸ¼ ì°¾ê¸°
        patterns = {
            "ê³¼ì¤‘": r"^ê³¼ì¤‘",
            "ì¢…ê²½": r"^ì¢…ê²½",
            "íš¡ê²½": r"^íš¡ê²½",
            "ê²½ë„": r"ê²½ë„\s*í‰ê· |ê²½ë„í‰ê· ",       # ê²½ë„í‰ê· (N/Ã¸11mm) ë‹¤ì–‘í•œ í‘œê¸°
            "ë‹¹ë„": r"^ë‹¹ë„",
            "ì‚°ë„": r"^ì‚°ë„",
            "L":   r"Hunter\s*L\b",
            "a":   r"Hunter\s*a\b",
            "b":   r"Hunter\s*b\b",
        }

        rows = []
        for k, pat in patterns.items():
            col = get_first_col_by_pattern(last_row.to_frame().T, pat)
            last_val = None
            if col is not None:
                try:
                    last_val = float(str(last_row[col]).replace(",", "").strip())
                except Exception:
                    last_val = None
            pred_val = preds.get(k, None)
            rows.append([k, pred_val, last_val])

        compare_df = pd.DataFrame(rows, columns=["í•­ëª©","ì˜ˆì¸¡ê°’(ì˜¬í•´)","ì „ë…„ë„ ì‹¤ì œê°’"])
        st.subheader(f"{cur_year} ì˜ˆì¸¡ vs ì „ë…„ë„ ê³¼ì‹¤í’ˆì§ˆ")
        st.dataframe(compare_df, use_container_width=True)

        plot_df = compare_df.dropna(subset=["ì˜ˆì¸¡ê°’(ì˜¬í•´)","ì „ë…„ë„ ì‹¤ì œê°’"]).copy()
        if not plot_df.empty:
            fig, ax = plt.subplots(figsize=(8, 4))
            x = np.arange(len(plot_df))
            w = 0.35
            ax.bar(x - w/2, plot_df["ì˜ˆì¸¡ê°’(ì˜¬í•´)"].values, width=w, label="ì˜ˆì¸¡(ì˜¬í•´)")
            ax.bar(x + w/2, plot_df["ì „ë…„ë„ ì‹¤ì œê°’"].values, width=w, label="ì „ë…„ë„")
            ax.set_xticks(x)
            ax.set_xticklabels(plot_df["í•­ëª©"].tolist(), rotation=0)
            ax.set_title(f"{region_disp} Â· {cultivar}  ì˜¬í•´ ì˜ˆì¸¡ vs ì „ë…„ë„")
            ax.legend()
            st.pyplot(fig)
        else:
            st.info("ê·¸ë˜í”„ë¡œ ë¹„êµí•  ìˆ˜ ìˆëŠ” ê³µí†µ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.warning("ì „ë…„ë„ ê³¼ì‹¤í’ˆì§ˆì—ì„œ í•´ë‹¹ ì§€ì—­ í–‰ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í’ˆì¢…/ì§€ì—­ ì¡°í•©ì„ ë°”ê¿”ë³´ì„¸ìš”.")

    # ì›ìë£Œ ë‹¤ìš´ë¡œë“œ
    c1, c2, c3 = st.columns(3)
    with c1:
        st.download_button(
            "â¬‡ï¸ ì˜¬í•´ ì‹¤ì¸¡ ì›”ë³„ CSV",
            df.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"{region}_{cultivar}_{cur_year}_monthly_measured.csv",
            mime="text/csv",
            disabled=df.empty
        )
    with c2:
        st.download_button(
            "â¬‡ï¸ ì˜¬í•´ ì˜ˆì¸¡ì— ì‚¬ìš©í•œ ì›”ë³„ CSV",
            filled_this_year.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"{region}_{cultivar}_{cur_year}_monthly_used.csv",
            mime="text/csv"
        )
    with c3:
        st.download_button(
            "â¬‡ï¸ íšŒê·€ì‹ ì˜ˆì¸¡ ê²°ê³¼ CSV",
            pred_df.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"{region}_{cultivar}_{cur_year}_predictions.csv",
            mime="text/csv"
        )
else:
    st.info("ì¢Œì¸¡ì—ì„œ ì§€ì—­ê³¼ í’ˆì¢…ì„ ê³ ë¥¸ ë’¤ ìë™ì¡°íšŒ & ì˜ˆì¸¡ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”. ì˜¬í•´ ë‚¨ì€ ì›”ì€ â€˜ì˜ˆìƒ ë‚ ì”¨(ìµœê·¼ 3ë…„ ë˜ëŠ” ì „ì²´ ê³¼ê±° í‰ê· )â€™ë¡œ ì±„ì›Œ ì˜ˆì¸¡í•©ë‹ˆë‹¤.")
